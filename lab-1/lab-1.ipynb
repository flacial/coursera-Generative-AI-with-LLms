{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c15108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices = [40, 200]\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)    \n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70dc9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e816947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"potato chips, cola\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "sentence_decoded = tokenizer.decode(\n",
    "  sentence_encoded[\"input_ids\"][0],\n",
    "  skip_speical_tokens=True\n",
    ")\n",
    "\n",
    "print('ENCODED SENTENCE:')\n",
    "# here its accessing an object and then a nested array.\n",
    "# didn't make sense to me first time.\n",
    "print(sentence_encoded['input_ids'][0])\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d93e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAIN ZERO SHOT INFERENCE\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=50,\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)    \n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION — WiTHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZERO SHOT INFERENCE with INSTRUCTION\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=50,\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)    \n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION — WiTHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7362b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    \n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\"\"\"\n",
    "        dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "        \n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cefdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example number 40\n",
    "example_indices_full = [40]\n",
    "# example number 200, it could be 30, 10, anything in the limits of the set\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b26ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE SHOT INFERENCE\n",
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)    \n",
    "print(f'MODEL GENERATION — WITH PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf5f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 40, 80, and 120\n",
    "example_indices_full = [40, 80, 120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a92d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEW SHOT INFERENCE\n",
    "generation_config = GenerationConfig(max_new_tokens=50)\n",
    "# Adjust temperature to mess with how random the completion is.\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=2.0)\n",
    "\n",
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)    \n",
    "print(f'MODEL GENERATION — WITH PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc72997",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ITS FINE TUNING TIME!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
